{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fad03564-4641-42fe-adce-b1555f255138",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "\n",
    "They apply a non-linear transformation and decide whether a neuron should be activated or not.\n",
    "\n",
    "if we don't apply activation functions, then basically the whole network is a stacked linear regression model\n",
    "\n",
    "So After each layer we typically use an activation function\n",
    "\n",
    "\n",
    "Most popular Activation functions\n",
    "1. Step Function\n",
    "2. Sigmoid\n",
    "3. tanH\n",
    "4. ReLU\n",
    "5. Leaky ReLU\n",
    "6. Softmax\n",
    "\n",
    "**Step Function**\n",
    "\n",
    "f(x) = 1 if x > 0\n",
    "\n",
    "f(x) = 0 otherwise\n",
    "\n",
    "\n",
    "**Sigmoid Function**\n",
    "\n",
    "f(x) = 1 / (1 + e^-x)\n",
    "\n",
    "Will output a probability between 0 and 1. Used in last layer of network\n",
    "\n",
    "**tanH Function**\n",
    "\n",
    "Scaled Sigmoid function. gives output between -1 and +1. Used in Hidden Layer\n",
    "\n",
    "f(x) = (2 / (1 + e^(-2x)))-1 \n",
    "\n",
    "\n",
    "**ReLU**\n",
    "\n",
    "most popular choice. if you don't know anything use ReLU in hidden Layer. \n",
    "\n",
    "f(x) = max(0,x)\n",
    "\n",
    "\n",
    "**Leaky ReLU**\n",
    "\n",
    "f(x) = x if x >= 0\n",
    "\n",
    "f(x) = a.x otherwise where a is very small value.\n",
    "\n",
    "Improved version of ReLU. Tries to solve the vanishing gradient problem. In case of ReLU, due to 0, backpropagation might be zero and the neuron's weight may not be updated. So try to use Leaky ReLU\n",
    "\n",
    "**Softmax**\n",
    "\n",
    "S(y(i)) = e ^ y(i) / sum(e^y(i))\n",
    "\n",
    "good choice in last layer in multi class classification problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9089b44e-c371-4b36-8240-9d8db26b887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ccf6ce6-82c0-41cf-91fb-1640d16fc8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1 --> create nn modules\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "397011c0-ba76-4159-9267-798b0a99ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2 --> use activation function directly in forward pass\n",
    "# option 1 --> create nn modules\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = torch.relu(self.linear1(x))\n",
    "        out = torch.sigmoid(self.linear2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d67a68-e522-47c1-8043-4f740f635c49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
